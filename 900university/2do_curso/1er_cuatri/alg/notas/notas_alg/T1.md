# Tema 1: Análisis de Algoritmos y Complejidad

Este documento resume los conceptos fundamentales del análisis de algoritmos, la medición de su eficiencia y las técnicas para verificarla, basándose en los ficheros `T1.1`, `T1.2` y `T1.3` de la asignatura.

## 1. Conceptos Fundamentales

### 1.1. Análisis de Eficiencia de Algoritmos

El objetivo principal del análisis de algoritmos es **predecir su comportamiento** y medir su eficiencia en términos de recursos computacionales. Los dos recursos más importantes son:

- **Tiempo de ejecución (Complejidad Temporal):** Cuánto tarda un algoritmo en completarse.
- **Memoria (Complejidad Espacial):** Cuánta memoria utiliza un algoritmo.

Para realizar un análisis teórico que sea independiente de la máquina, el lenguaje o el compilador, nos abstraemos en un **Modelo de Computación** (máquina de acceso aleatorio o RAM), donde asumimos que las instrucciones simples se ejecutan en un tiempo constante. A estas se les denomina **operaciones elementales**.

En el análisis de algoritmos **no se trabaja sobre una computadora real**, sino sobre un **modelo conceptual de computación** (modelo RAM) que **abstrae la complejidad del hardware real**.

En este modelo se asume que las **operaciones elementales tienen coste constante**, lo que permite ignorar detalles de implementación y centrarse en lo verdaderamente relevante:

**la tendencia de crecimiento del algoritmo cuando aumenta el tamaño del problema**, es decir, su **comportamiento asintótico**.

De este modo, el análisis mide la **eficiencia del algoritmo** en términos de **complejidad temporal y espacial**, proporcionando una base sólida para **comparar algoritmos de forma independiente de la máquina utilizada**.

### 1.2. Análisis de Casos

Un algoritmo puede no tardar siempre lo mismo para una entrada del mismo tamaño(El contenido y el orden de la entrada importan). Por ello, se distinguen tres tipos de análisis:

- **Peor Caso:** El tiempo máximo de ejecución para una entrada de tamaño `n`. Es el más utilizado porque nos da una cota superior garantizada.
- **Mejor Caso:** El tiempo mínimo de ejecución para una entrada de tamaño `n` (cota mínima).
- **Caso Promedio:** El tiempo de ejecución esperado para una entrada aleatoria de tamaño `n`. Es más difícil de calcular.

### 1.3. Notaciones Asintóticas

Las notaciones asintóticas nos permiten describir el comportamiento de un algoritmo cuando el tamaño de la entrada `n` tiende a infinito. Ignoramos las constantes y los términos de orden inferior.

- **Notación O (Big O - Cota Superior):**
  - **Definición:** `T(n) = O(f(n))` si existen constantes `c > 0` y `n₀ > 0` tales que `T(n) ≤ c * f(n)` para todo `n ≥ n₀`.
  - **Significado:** El tiempo de ejecución `T(n)` crece, en el peor de los casos, a un ritmo que no supera al de `f(n)`.

- **Notación Ω (Big Omega - Cota Inferior):**
  - **Definición:** `T(n) = Ω(f(n))` si existen constantes `c > 0` y `n₀ > 0` tales que `T(n) ≥ c * f(n)` para todo `n ≥ n₀`.
  - **Significado:** El tiempo de ejecución `T(n)` crece, como mínimo, al mismo ritmo que `f(n)`.

- **Notación Θ (Big Theta - Cota Ajustada):**
  - **Definición:** `T(n) = Θ(f(n))` si `T(n) = O(f(n))` y `T(n) = Ω(f(n))`. Esto es, si existen constantes `c₁, c₂ > 0` y `n₀ > 0` tales que `c₁ * f(n) ≤ T(n) ≤ c₂ * f(n)` para todo `n ≥ n₀`.
  - **Significado:** El tiempo de ejecución `T(n)` crece exactamente al mismo ritmo que `f(n)`. Es la cota más precisa.

### 1.4. Reglas para el Cálculo de Complejidad

- **Regla de la Suma (Secuencias):** Para una secuencia de dos bloques de código, la complejidad es la del bloque con mayor complejidad.
  `O(f(n)) + O(g(n)) = O(max(f(n), g(n)))`
- **Regla del Producto (Bucles):** Para un bucle cuyo cuerpo tiene complejidad `O(f(n))` y se ejecuta `O(g(n))` veces, la complejidad total es el producto.
  `O(f(n)) * O(g(n)) = O(f(n) * g(n))`

### 1.5. Relaciones de Recurrencia y el Teorema Maestro(Teorema de divide y vencerás)

Los algoritmos recursivos se analizan mediante relaciones de recurrencia. Una forma común es la de los algoritmos de **Divide y Vencerás**:

`T(n) = l * T(n/b) + f(n)`

Donde:
- `l`: número de subproblemas.
- `n/b`: tamaño de cada subproblema.
- `f(n)`: coste de dividir el problema y combinar las soluciones.

El **Teorema Maestro(divide y vencerás)** nos da una solución directa para muchas de estas recurrencias:
"La forma tradiccional en que lo escriben es: `T(n) = l * T(n/b) + c*n^k, n>n0`", donde:
- L > 0 -> numero de subproblemas
- b > 1 -> tamaño de la division del problema en cada subproblema
- n0 > 0 -> umbral del problema, pasado cierto umbral se satisface que T(n) acotada
- c>0,k>0.

>n0,c,k vienen englobados en f(n) de ahi que sea valido decir "$T(n) = \ell \cdot T(n/b) + f(n)$"
   Dada `T(n) = l * T(n/b) + O(n^k)`, podemos comparar `l` con `b^k`:

1.  **Si `l > b^k`**: `T(n) = Θ(n^(log_b(l)))`
2.  **Si `l = b^k`**: `T(n) = Θ(n^k * log n)`
3.  **Si `l < b^k`**: `T(n) = Θ(n^k)`
### 1.6. Verificación Empírica

Complementa el análisis teórico. Consiste en:
1.  **Hipotetizar** una cota de complejidad `f(n)` (ej. `n²`, `n log n`).
2.  **Medir** los tiempos de ejecución `t(n)` para tamaños de entrada `n` que sigan una progresión geométrica (ej. 1000, 2000, 4000...).
3.  **Calcular la serie de ratios:** `t(n) / f(n)`.
4.  **Analizar la convergencia:**
    - Si `t(n) / f(n) → C > 0` (una constante), la cota es ajustada: `T(n) = Θ(f(n))`.
    - Si `t(n) / f(n) → ∞` (diverge), la cota está subestimada.
    - Si `t(n) / f(n) → 0` (decrece), la cota está sobrestimada.

---

## 2. Ejercicio Guiado: Análisis Completo de la Exponenciación Recursiva

Vamos a aplicar todos los conceptos anteriores al algoritmo `Potencia2` visto en las diapositivas (T1.1, pág. 26).

El algoritmo calcula `x^n` de forma recursiva:
- Si `n` es par: `x^n = (x^(n/2)) * (x^(n/2))`
- Si `n` es impar: `x^n = x * (x^((n-1)/2)) * (x^((n-1)/2))`

### Parte 1: Análisis Teórico

#### Algoritmo

**Pseudocódigo (`Potencia2`)**
```
función potencia(x, n)
  si n = 0 entonces
    devolver 1
  sino si n es par entonces
    res <- potencia(x, n/2)
    devolver res * res
  sino
    res <- potencia(x, (n-1)/2)
    devolver x * res * res
  fin si
fin función
```

**Implementación en Python**
```python
def potencia(x, n):
  if n == 0:
    return 1
  elif n % 2 == 0:
    res = potencia(x, n // 2)
    return res * res
  else:
    res = potencia(x, (n - 1) // 2)
    return x * res * res
```

#### Análisis de Complejidad

La operación principal que contamos es la multiplicación.

1.  **Establecer la Relación de Recurrencia:**
    - En cada llamada, se realiza una llamada recursiva a un problema de tamaño aproximadamente `n/2`.
    - Después de la llamada, se realizan 1 o 2 multiplicaciones. Esto es un coste constante, `O(1)`.
    - Por tanto, la recurrencia es: `T(n) = T(n/2) + O(1)`

2.  **Resolver con el Teorema Maestro:**
    - La recurrencia es de la forma `T(n) = a * T(n/b) + O(n^k)`.
    - Identificamos los parámetros:
      - `a = 1` (una llamada recursiva)
      - `b = 2` (el tamaño se divide por 2)
      - `k = 0` (porque `O(1) = O(n^0)`)
    - Comparamos `a` con `b^k`: `1` vs `2^0`, es decir, `1` vs `1`.
    - Estamos en el **Caso 2** del Teorema Maestro (`a = b^k`).
    - La solución es `T(n) = Θ(n^k * log n) = Θ(n^0 * log n) = Θ(log n)`.

**Conclusión Teórica:** La complejidad temporal de la exponenciación recursiva es **`Θ(log n)`**.

### Parte 2: Verificación Empírica

Ahora, comprobamos si nuestro análisis `Θ(log n)` se sostiene en la práctica.

#### Script de Medición

```python
import time
import math

# (Aquí iría la función potencia(x, n) definida antes)

def medir_tiempo():
    # Tamaños de n en progresión geométrica (aproximada)
    ns = [10**3, 10**4, 10**5, 10**6, 10**7, 10**8]
    x = 2 # Base arbitraria
    
    print(f"{'n':>10s} {'t(n) (ns)':>15s} {'f(n) = log(n)':>15s} {'t(n)/f(n)':>15s}")
    print("-" * 60)

    for n in ns:
        # Medir tiempos pequeños requiere repetir la ejecución
        k = 1000
        
        # Medimos el tiempo de K ejecuciones para mayor precisión
        t_start = time.perf_counter_ns()
        for _ in range(k):
            potencia(x, n)
        t_end = time.perf_counter_ns()
        
        # Tiempo promedio de una ejecución
        t_total_ns = (t_end - t_start) / k
        
        # Hipótesis: f(n) = log(n)
        f_n = math.log(n)
        
        if f_n == 0: continue
        
        ratio = t_total_ns / f_n
        
        print(f"{n:10d} {t_total_ns:15.2f} {f_n:15.2f} {ratio:15.2f}")

# Para ejecutar:
# medir_tiempo()
```

#### Resultados y Discusión (Ejemplo)

Al ejecutar un script como el anterior, obtendríamos una tabla similar a esta (los tiempos son ilustrativos):

| n        | t(n) (ns) | f(n) = log(n) | t(n)/f(n) |
|----------|-----------|---------------|-----------|
| 1000     | 205.30    | 6.91          | 29.71     |
| 10000    | 270.10    | 9.21          | 29.32     |
| 100000   | 335.80    | 11.51         | 29.17     |
| 1000000  | 402.50    | 13.82         | 29.13     |
| 10000000 | 465.10    | 16.12         | 28.85     |
| 100000000| 530.90    | 18.42         | 28.82     |

**Análisis de la tabla:**
La columna `t(n)/f(n)` muestra la convergencia de los ratios a una constante `C` que es aproximadamente `29`. Como el ratio converge a una constante positiva, se confirma empíricamente que nuestra hipótesis era correcta.

**Conclusión Final:** `T(n) = Θ(log n)`.

---

## 3. Algoritmos de Ordenación Básicos

### 3.1. Ordenación por Inserción

**Pseudocódigo** (T1.1, pág. 20)
```
procedimiento Ordenación por Inserción (var T[1..n])
  para i:=2 hasta n hacer
    x:=T[i];
    j:=i-1;
    mientras j>0 y T[j]>x hacer
      T[j+1]:=T[j];
      j:=j-1
    fin mientras;
    T[j+1]:=x
  fin para
fin procedimiento
```

**Implementación en Python**
```python
def insertion_sort(arr):
  # Empezamos desde el segundo elemento (índice 1)
  for i in range(1, len(arr)):
    key = arr[i]
    j = i - 1
    # Mover los elementos de arr[0..i-1] que son mayores que key
    # a una posición adelante de su posición actual
    while j >= 0 and arr[j] > key:
      arr[j + 1] = arr[j]
      j -= 1
    arr[j + 1] = key
  return arr
```
**Análisis de Complejidad:**
- **Peor Caso** (vector ordenado inversamente): `Θ(n²)`
- **Mejor Caso** (vector ya ordenado): `Θ(n)`

### 3.2. Ordenación por Selección

**Pseudocódigo** (T1.1, pág. 23)
```
procedimiento Ordenación por Selección (var T[1..n])
  para i:=1 hasta n-1 hacer
    minj:=i;
    minx:=T[i];
    para j:=i+1 hasta n hacer
      si T[j]<minx entonces
        minj:=j;
        minx:=T[j]
      fin si
    fin para;
    T[minj]:=T[i];
    T[i]:=minx
  fin para
fin procedimiento
```

**Implementación en Python**
```python
def selection_sort(arr):
  n = len(arr)
  # Recorrer todo el array
  for i in range(n):
    # Encontrar el mínimo en el resto del array sin ordenar
    min_idx = i
    for j in range(i + 1, n):
      if arr[j] < arr[min_idx]:
        min_idx = j
    
    # Intercambiar el mínimo encontrado con el primer elemento del subarray
    arr[i], arr[min_idx] = arr[min_idx], arr[i]
  return arr
```
**Análisis de Complejidad:**
- **Peor Caso, Mejor Caso y Caso Promedio:** `Θ(n²)` en todos los casos, ya que siempre realiza el mismo número de comparaciones.
